var relearn_searchindex = [
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop \u003e Query and Visualize",
    "content": "Steps Prerequisites Getting Started Query Data with Amazon Athena Build an Amazon QuickSight Dashboard Set up QuickSight Create QuickSight Charts Create QuickSight Parameters Create a QuickSight Filter Prerequisites Note Ingestion with Glue and Transforming data with Glue ETL labs are prerequisites for this lab.\nGetting Started In this lab, you will complete the following tasks:\nQuery data and create a view with Amazon Athena Athena Workgroups to Control Query Access and Costs Build a dashboard with Amazon QuickSight Query Data with Amazon Athena In the AWS services console, search for Athena. In the Query Editor, select your newly created database e.g., “ticketdata”.\nClick the table named “parquet_sporting_event_ticket” to inspect the fields. Note: The type for fields id, sporting_event_id and ticketholder_id should be (double). Next, we will query across tables parquet_sporting_event, parquet_sport_team, and parquet_sport location.\nCopy the following SQL syntax into the Query 1 tab and click Run. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT e.id AS event_id, e.sport_type_name AS sport, e.start_date_time AS event_date_time, h.name AS home_team, a.name AS away_team, l.name AS location, l.city FROM parquet_sporting_event e, parquet_sport_team h, parquet_sport_team a, parquet_sport_location l WHERE e.home_team_id = h.id AND e.away_team_id = a.id AND e.location_id = l.id; The results appear beneath the query window. As shown above Click Create and then select View from query\nName the view sporting_event_info and click Create Your new view is created Copy the following SQL syntax into the Query 3 tab 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 SELECT t.id AS ticket_id, e.event_id, e.sport, e.event_date_time, e.home_team, e.away_team, e.location, e.city, t.seat_level, t.seat_section, t.seat_row, t.seat, t.ticket_price, p.full_name AS ticketholder FROM sporting_event_info e, parquet_sporting_event_ticket t, parquet_person p WHERE t.sporting_event_id = e.event_id AND t.ticketholder_id = p.id Click on Save as button Give this query a name: create_view_sporting_event_ticket_info and some description and then, click on Save query. Back to the query editor, you will see the query name changed. Now, click on Run The results appear beneath the query window. As shown above, click View from query.\nName the view sporting_event_ticket_info and click Create. Copy the following SQL syntax into the Query 5 tab. 1 2 3 4 5 6 7 8 9 SELECT sport, count(distinct location) as locations, count(distinct event_id) as events, count(*) as tickets, avg(ticket_price) as avg_ticket_price FROM sporting_event_ticket_info GROUP BY 1 ORDER BY 1; Click on Save as and give this query name: analytics_sporting_event_ticket_info and some description and then, click on Save. The name of the New Query 5 will be changed to one assigned in previous step. Click on Run.\nYou query returns two results in approximately five seconds. The query scans 25 MB of data, which prior to converting to parquet, would have been 1.59GB of CSV files. The purpose of saving the queries is to have clear distinction between the results of the queries running on one view. Otherwise, your query results will be saved under “Unsaved” folder within the S3 bucket location provided to Athena to store query results. Please navigate to S3 bucket to observe these changes, as shown below: Build an Amazon QuickSight Dashboard Set up QuickSight In the AWS services console, search for QuickSight. If this is the first time you have used QuickSight, you are prompted to create an account.\nClick Sign up for QuickSight.\nFor account type, choose the default Enterprise Version.\nClick Continue.\nOn the Create your QuickSight account page, select No, Maybe Later for the Paginated Report add-on. On the new screen, leave the default selection for authentication method.\nNext choose the appropriate AWS region based on where you are running this workshop on and the check boxes to enable auto discovery, Amazon Athena, and Amazon S3. For QuickSight account name give a unique name (e.g., quicksight-lab--) and email address.\nSelect your DMS bucket (e.g., “xxx-dmslabs3bucket-xxx”), Click Finish. Please click Finish\nOn the top right corner, click New analysis. Click New dataset. On the Create a Dataset page, select Athena as the data source. For Data source name, type ticketdata-qs , then click Validate connection.\nClick Create data source. In the Database drop-down list, select the database ticketdata.\nChoose the “sporting_event_ticket_info” table and click Select. To finish data set creation, choose the option Import to SPICE for quicker analytics and click Visualize. If your SPICE has 0 bytes available, choose the second choice Directly query your data. Create QuickSight Charts In this section we will take you through some of the different chart types.\nIn the Fields list, click the ticket_price column to populate the chart.\nClick the expand icon in corner of “ticket_price” field, and select Show as Currency to show the number in dollar value. You can add visual by clicking Add button from the Visuals pane of the screen.\nIn the Visual types area, choose the Vertical bar chart icon. This layout requires a value for the X-axis. In Fields list, select the event_date_time field and you should see the visualization update. For Value Y-axis, select “ticket_price” from the Field list. You can drag and move other visuals to adjust space in dashboard. In the Fields list, click and drag the seat_level field to the Group/Color box. You can also use the slider below the x axis to fit all of the data. Let’s build on this one step further by changing the chart type:\nIn the Visuals pane, click Vertical bar chart under change visual type and choose the Clustered bar combo chart icon.\nIn the Fields list, click and drag the ticketholder field to the Lines box.\nIn the Lines box, click the dropdown box and choose Aggregate: Count Distinct for Aggregate. You can then see the y-axis update on the right-hand side. Click on insight icon on the Tool bar and explore insight information in simple English. Feel free to experiment with other chart types and different fields to get a sense of the data.\nCreate QuickSight Parameters In the next section we are going to create some parameters with controls for the dashboard, then assign these to a filter for all the visuals.\nFrom the Tool bar, select Parameters icon. Click Add to create a new parameter with a Name.\nFor Name, type EventFrom.\nFor Data type, choose Datetime.\nFor Time granularity, set Hour.\nFor Default value, select the value from calendar as start date available in your graph for event_date_time. For example, 2021-01-01 00:00.\nClick Create, and then close the Parameter Added dialog box. Create another parameter with the following attributes:\nName: EventTo Data type: Datetime For Time granularity, set Hour. For Default value, select the value from calendar as end date available in your graph for event_date_time. For example, 2022-01-01 00:00 Click Create In next window, you can select any option to perform any operation with the parameter. Alternatively, you can click the three dots for the EventFrom parameter and choose Add control. For Display name, specify Event From and click Add. Repeat the process to add a control for EventTo with display name Event To You should now be able to see and expand the Controls section above the chart. Create a QuickSight Filter To complete the process, we will wire up a filter to these controls for all visuals.\nFrom the Tool bar, choose Filter.\nClick the plus icon (+) to add a filter for the field event_date_time. Click this filter to edit the properties.\nSelect All applicable visuals in the Applied To dropdown\nFor Filter type, choose Date \u0026 Time range and Condition as Between.\nSelect option Use Parameter, click Yes to apply to all visual.\nFor Start date parameter, choose EventFrom.\nFor End date parameter, choose EventTo. Click Apply.\nYou can now click on Publish on the top right corner of screen to publish your visuals. You can also, share it by clicking on the Share menu.\nA dashboard is a read-only snapshot of an analysis that you can share with other Amazon QuickSight users for reporting purposes. In Dashboard other users can still play with visuals and data but that will not modify dataset.\nYou can share an analysis with one or more other users with whom you want to collaborate on creating visuals. Analysis provides other uses to write and modify data set.",
    "description": "Steps Prerequisites Getting Started Query Data with Amazon Athena Build an Amazon QuickSight Dashboard Set up QuickSight Create QuickSight Charts Create QuickSight Parameters Create a QuickSight Filter Prerequisites Note Ingestion with Glue and Transforming data with Glue ETL labs are prerequisites for this lab.\nGetting Started In this lab, you will complete the following tasks:\nQuery data and create a view with Amazon Athena Athena Workgroups to Control Query Access and Costs Build a dashboard with Amazon QuickSight Query Data with Amazon Athena In the AWS services console, search for Athena.",
    "tags": [],
    "title": "Athena and QuickSight",
    "uri": "/5-queryandvisualize/5.1/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Overview Data Engineering Immersion Day Workshop\nWhat is a Data Engineering Immersion Day? The Data Engineering Immersion Day has hands-on labs and modules that focus on ingestion, hydration, exploration, and consumption of data in a data lake in AWS.\nBenefits of a Data Engineering Immersion Day. The Data Engineering Immersion day allows hands-on time with AWS analytics services including Amazon Kinesis Services for streaming data ingestion and analytics, AWS Data Migration service for batch data ingestion, AWS Glue for data catalog and run ETL on Data lake, Amazon Athena to query data lake, and Amazon Quicksight for visualization. This Immersion day helps to build a cloud-native and future-proof serverless data lake.\nContent Introduction Prepare Data Ingestion Transforming data with Glue Query and Visualize Clean resource Summary",
    "description": "Overview Data Engineering Immersion Day Workshop\nWhat is a Data Engineering Immersion Day? The Data Engineering Immersion Day has hands-on labs and modules that focus on ingestion, hydration, exploration, and consumption of data in a data lake in AWS.\nBenefits of a Data Engineering Immersion Day. The Data Engineering Immersion day allows hands-on time with AWS analytics services including Amazon Kinesis Services for streaming data ingestion and analytics, AWS Data Migration service for batch data ingestion, AWS Glue for data catalog and run ETL on Data lake, Amazon Athena to query data lake, and Amazon Quicksight for visualization. This Immersion day helps to build a cloud-native and future-proof serverless data lake.",
    "tags": [],
    "title": "Data Engineering Immersion Day Workshop",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop \u003e Data Ingestion",
    "content": "Steps Create S3 bucket. Open AWS CloudShell. Copy the data from the staging Amazon S3 bucket to your S3 bucket. Verify the data. Step 1: Create S3 bucket Create S3 bucket to store data.\nBucket name: ingestion-data-skip-dms-lab Success to create S3 bucket Step 2: Open AWS CloudShell Open AWS CloudShell. It will open a terminal window in the browser. (If there is a pop-up, close it)\nNote CloudShell is available in specific regions and you shall find the supported regions in AWS Regional Services List. By executing the following command you will be copying the data to the correct S3 bucket in whatever region it belongs (It can also be across region)\nStep 3: Copy the data from the staging Amazon S3 bucket to your S3 bucket Issue the following command in the terminal, and replace the bucket name with your own one. aws s3 cp --recursive --copy-props none s3://aws-dataengineering-day.workshop.aws/data/ s3://\u003cYourBucketName\u003e/tickets/\nStep 4: Verify the data Open the S3 console and view the data that was copied through CloudShell terminal. Your S3 bucket name will look like below: BucketName/bucket_folder_name/schema_name/table_name/objects/ In our lab example this becomes: /‘BucketName’/tickets/dms_sample with a separate path for each table_name Next Steps In the next part of this lab, we will complete the following tasks:\nExtract, Transform and Load Data Lake with AWS Glue",
    "description": "Steps Create S3 bucket. Open AWS CloudShell. Copy the data from the staging Amazon S3 bucket to your S3 bucket. Verify the data. Step 1: Create S3 bucket Create S3 bucket to store data.\nBucket name: ingestion-data-skip-dms-lab",
    "tags": [],
    "title": "Data Ingestion With AWS S3",
    "uri": "/3-ingestion/3.1/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop \u003e Transforming Data",
    "content": "PART A: Create Glue Crawler for initial full load data Navigate to the AWS Glue Console On the AWS Glue menu, under ‘Data Catalog’, select Crawlers. Click Create crawler.\nEnter glue-lab-crawler as the crawler name for initial data load.\nOptionally, enter the description. This should also be descriptive and easily recognized and Click Next. Under Choose data sources and classifiers select Not yet and click Add a data source 7.On the Add a data store page, make the following selections:\nFor Data source, click the drop-down box and select S3. For Location of S3 data, select In this account For S3 path, browse to the target folder stored CSV files, e.g., s3://ingestion-data-skip-dms-lab/tickets/ Leave all other parameters as default Click Add an S3 data source Click Next On the Configure security settings page, make the following selection:\nUnder Existing IAM role select AWSGlueServiceRoleDefault which is pre-created for this lab. Information about AWSGlueServiceRoleDefault Click Next On the Set output and scheduling, under Target database, click Add database which pops-up another tab.\nEnter ticketdata as your database name and click Create database Please get back to the Set output and scheduling page Select Target database as ’ticketdata’. Please use the refresh option right next to the dropdown to make sure that the new database we created is listing there. For Prefix added to tables (optional), leave the field empty. Under Crawler schedule as select the frequency as ‘On demand’ Click Next Review the information and click Create crawler. The crawler is now ready to run. Execute the Crawler by clicking Run crawler button. Crawler will change status from ‘Running’ to ‘Stopping’, wait until crawler comes back to ‘Ready’ state (the process will take a few minutes), you can see that it has created 15 tables. In the AWS Glue navigation pane, under Data Catalog click Databases → Tables. You can also click the ticketdata database to browse the tables. PART B: Data Validation Exercise Within the Tables section of your ticketdata database, click the person table. You may have noticed that some tables (such as person) have column headers such as col0,col1,col2,col3. In absence of headers or when the crawler cannot determine the header type, default column headers are specified. This exercise uses the person table in an example of how to resolve this issue. Click Edit Schema option In the Edit Schema section, select the row for col0 (column name) and click Edit.\nType id as the column name and click Save. Repeat the above steps to change the remaining column names to match those shown in the following figure: full_name, last_name and first_name. Click Save as new table version. PART C: Data ETL Exercise Pre-requisite: To store processed data in parquet format, we need a new folder location for each table, eg. the full path for sport_team table look like this - s3://\u003cs3_bucket_name\u003e/tickets/dms_parquet/sport_team\nIn the left navigation pane, click ETL jobs.\nClick “Visual ETL” Select Amazon S3 from the Sources list to add a Data source - S3 bucket node. Select the Data source - S3 bucket node to show the data source properties.\nIn the panel on the right under “Data source properties - S3”, select Data Catalog table and choose the ticketdata database from the drop down. For Table, select the sport_team table. Click on the + button and select Change Schema from the Transforms list to add a Transform - Change Schema node. Select the Transform - Change Schema node to show the properties. In the Transform panel on the right, change the data type of id column to double in the dropdown. Click on the + button and select Amazon S3 from the Targets list to add a Data target - S3 bucket node. Select the Data target - S3 bucket node to show the properties. In the panel on the right, change the Format to Parquet in the dropdown. Under Compression Type, select Uncompressed from the dropdown.\nUnder “S3 Target Location”, select “Browse S3” browse to the “ingestion-data-skip-dms-lab” bucket, select “tickets” item and press “Choose”.\nIn the textbox, append dms_parquet/sport_team/ to the S3 url. The path should look similar to s3://ingestion-data-skip-dms-lab/tickets/dms_parquet/sport_team/ - don’t forget the / at the end. The job will automatically create the folder. Finally, select the Job details tab at the top. Enter Glue-Lab-SportTeamParquet under Name.\nFor IAM Role, select the role named similar to AWSGlueServiceRoleDefault. Scroll down the page and under Job bookmark, select Disable in the drop down. You can try out the bookmark functionality later in this lab.\nPress the Save button in the top right-hand corner to create the job.\nOnce you see the Successfully created job message in the banner, click the Run button to start the job.\nSelect Jobs from the navigation panel on the left-hand side to see a list of your jobs.\nSelect Monitoring from the navigation panel on the left-hand side to view your running jobs, success/failure rates and various other statistics.\nScroll down to the Job runs list to verify that the ETL job has completed successfully. This should take about 1 minute to complete. We need to repeat this process for an additional 4 jobs, to transform the sport_location, sporting_event, sporting_event_ticket and person tables.\nDuring this process, we will need to modify different column data types. We can either repeat the process above for each table, or we can clone the first job and update the details. The steps below describe how to clone the job - if creating manually each time, follow the above steps but make sure you use the updated values from the tables below. Return to the Jobs menu, and select the Glue-Lab-SportsTeamParquet job by clicking the checkbox next to the name. Under the Actions dropdown, select Clone job. Update the job as per the following tables, then Save and Run.\nSport_Location: Create a Glue-Lab-SportLocationParquet job with the following attributes: Sporting_Event: Create a Glue-Lab-SportingEventParquet job with the following attributes: Sporting_Event_Ticket: Create a Glue-Lab-SportingEventTicketParquet job with the following attributes: Person: Create a Glue-Lab-PersonParquet job with the following attributes: Success run all ETL job S3 Bucket for Parquet file is created PART D: Create Glue Crawler for Parquet Files In the Glue navigation menu, under Data Catalog select Crawlers. Click Create crawler. For Crawler name, type glue-lab-parquet-crawler and Click Next. In next screen Choose data sources and classifiers, select Not yet and click Add a data source\nIn the Add data store screen\nFor Choose a data store, select “S3”. For Location of S3 data, select “In this account”. For S3 path, specify the S3 Path (Parent Parquet folder) that contains the nested parquet files e.g., s3://xxx-dmslabs3bucket-xxx/tickets/dms_parquet/ Leave all other parameters as default Click Add an S3 data source. In next screen Choose data sources and classifiers, click Next. On the Configure security settings page, select Choose an existing IAM role. For IAM role, select the existing role “xxx-GlueLabRole-xxx” and Click Next. On the Set output and scheduling page, Under Target database, choose your existing database which you created earlier e.g. ticketdata For the Prefix added to tables (optional), type parquet_ For Crawler schedule select “On Demand” and Click Next. Review the summary page and click Create crawler. Click Run Crawler. Once your crawler has finished running, you should see that that tables were added, depending on how many parquet ETL conversions you set up in the previous section. Confirm you can see the tables\nIn the left navigation pane, click Tables. Add the filter parquet for Classification to return the newly created tables. Congratulations! You have completed the Data Validation and ETL lab",
    "description": "PART A: Create Glue Crawler for initial full load data Navigate to the AWS Glue Console On the AWS Glue menu, under ‘Data Catalog’, select Crawlers. Click Create crawler.\nEnter glue-lab-crawler as the crawler name for initial data load.\nOptionally, enter the description. This should also be descriptive and easily recognized and Click Next.",
    "tags": [],
    "title": "Data Validation and ETL With Glue",
    "uri": "/4-transforming/4.1/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "What you’ll do in these labs? These labs are designed to be completed in sequence, and the full set of instructions are documented below. Read and follow along to complete the labs.\nArchitectures Patterns Initially you will perform Data Ingestion Perform Data Transformations Explore the DataLake using SQL and Visualization tools Finally you will apply Machine Learning",
    "description": "What you’ll do in these labs? These labs are designed to be completed in sequence, and the full set of instructions are documented below. Read and follow along to complete the labs.\nArchitectures Patterns Initially you will perform Data Ingestion Perform Data Transformations Explore the DataLake using SQL and Visualization tools",
    "tags": [],
    "title": "Introduction",
    "uri": "/1-introduction/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop \u003e Query and Visualize",
    "content": "Introduction Amazon SageMaker is an end-to-end machine learning platform that lets you build, train, and deploy machine learning models in AWS. It is a highly modular service that lets you use each of these components independently of each other.\nYou will Learn:\nHow to use Jupyter notebooks in SageMaker to integrate with the data lake using Athena Populate data frames for data manipulation. This process to prepare the data to satisfy the needs of ML algorithms is iterative. To prepare the data, we will make the table definitions in Athena available in a Jupyter notebook in SageMaker.\nJupyter notebooks are popular among data scientists and used to visualize data, perform statistical analysis, complete data manipulations, and make the data ready for machine learning work.\nNote Prerequisites\nIngestion with Glue and Transforming data with Glue ETL labs are prerequisites for this lab.\nCreate Amazon SageMaker Notebook Instance Go to the Amazon SageMaker from AWS console In the Amazon SageMaker navigation pane, go to Notebook instances and click Create notebook instance. Enter following values to create the notebook instance\nEnter a name for the Notebook instance e.g. datalake-Sagemaker\nYou can leave Notebook instance type as the default value ml.t3.medium for this lab\nLeave Elastic Inference as none. This is to add extra resources.\nUnder IAM role, we need to choose a role for the notebook instance in Amazon SageMaker to interact with Amazon S3. As the role doesn’t exist, from the dropdown option select Create a new role.\nIn the Create an IAM role pop up window, choose Any S3 bucket as shown below and click Create role. After the IAM role is created, click on the link to the IAM role, this will open the IAM role in a new tab. We will add IAM permissions to allow access to Athena from SageMaker. Click on Add permissions and select Attach policies.\nFilter policies by “Athena”, check AmazonAthenaFullAccess managed policy and click Attach policies button at the bottom of the screen. Go back to the SageMaker Create notebook instance browser tab.\nLeave all other options with default values. Click Create notebook instance.\nWait for the notebook instance to be created and the Status to change to InService. Click the Open Jupyter link in the Actions column. The notebook interface opens in a new tab. Connect the SageMaker Jupyter notebook to Athena In the Jupyter notebook tab, click New and select conda_python3 for the kernel. This opens a notebook in a new tab. Note Amazon SageMaker provides several kernels for Jupyter, including support for Python 3, MXNet, TensorFlow, and PySpark. This exercise uses Python because it includes the pandas library.\nWithin the notebook, enter the following command into a cell and click Run to install the Athena JDBC driver. (PyAthena is a Python DB API 2.0 (PEP 249) compliant client for the Amazon Athena JDBC driver.) 1 !pip install PyAthena[SQLAlchemy] Check the output that PyAthena has been installed successfully. If you see errors related to awscli 1.x, they can be ignored.\nWorking in Pandas After the Athena driver is installed, you can use the JDBC connection to connect to Athena and populate the pandas dataframes. For data scientists, working with data is typically divided into multiple stages: ingesting and cleaning data, analyzing and modeling data, then organizing the results of the analysis into a form suitable for plotting or tabular display. Pandas is the ideal tool for all of these tasks.\nYou can load Athena table data from the data lake into a pandas dataframe and apply machine learning. Copy the following code into your notebook cell and replace with your region, S3 bucket location and database name.\nYour region: run the following command in the notebook cell to get the current AWS region. 1 !aws configure get region Your Athena Database name, this is the Glue Database name which you created during the Glue lab e.g. ticketdata. Update the S3 bucket location whose name contains string ingestion-data-skip-dms-lab followed by athena/, so that it looks like s3://ingestion-data-skip-dms-lab/athena/. 1 2 3 4 5 6 7 8 9 10 from sqlalchemy import create_engine import pandas as pd s3_staging_dir = \"s3://ingestion-data-skip-dms-lab/athena/\" connection_string = f\"awsathena+rest://:@athena.us-east-1.amazonaws.com:443/ticketdata?s3_staging_dir={s3_staging_dir}\" engine = create_engine(connection_string) df = pd.read_sql('SELECT * FROM \"ticketdata\".\"nfl_stadium_data\" order by stadium limit 10;', engine) df Click Run and dataframe will display query output. In this query, you are loading all nfl stadium information into a pandas dataframe from the table nfl_stadium_data.\nNote If you get a SageMaker does not have Athena execution permissions error issue. You need to add Athena Access to the Sagemaker role as steps provide in previous section.\nYou can apply different ML algorithms in the populated pandas dataframes. For example, draw a plot of the data. Copy the following code into your notebook. In this query, you are loading all even ticket information into a pandas dataframe from the table sporting_event_ticket_info. 1 2 3 4 5 6 7 8 9 10 11 12 13 df = pd.read_sql('SELECT sport, \\ event_date_time, \\ home_team,away_team, \\ city, \\ count(*) as tickets, \\ sum(ticket_price) as total_tickets_amt, \\ avg(ticket_price) as avg_ticket_price, \\ max(ticket_price) as max_ticket_price, \\ min(ticket_price) as min_ticket_price \\ FROM \"ticketdata\".\"sporting_event_ticket_info\" \\ group by 1,2,3,4,5 \\ order by 1,2,3,4,5 limit 1000;', engine) df Click Run and the data frame will display the query output. In a new execution cell, copy following code 1 2 import matplotlib.pyplot as plt df.plot(x='event_date_time',y='avg_ticket_price') Click Run and you will see data plot which got draw using matplotlib library. Cleanup Go to SageMaker Notebook Instances console Select the notebook instance that you created e.g. datalake-Sagemaker. Choose Actions → Stop, it will take a few minutes for the notebook instance to stop. Once the notebook instance is stopped, choose Actions → Delete and choose Delete in the confirmation pop-up to delete the instance.",
    "description": "Introduction Amazon SageMaker is an end-to-end machine learning platform that lets you build, train, and deploy machine learning models in AWS. It is a highly modular service that lets you use each of these components independently of each other.\nYou will Learn:\nHow to use Jupyter notebooks in SageMaker to integrate with the data lake using Athena Populate data frames for data manipulation.",
    "tags": [],
    "title": "Athena and SageMaker (Optional)",
    "uri": "/5-queryandvisualize/5.2/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "AWS Account and IAM User In order to get started working with AWS services, you will need an AWS account. You will also need an IAM user for this account that you can use to log into the AWS Management Console in order to provision and configure you resources.\nAWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny access to AWS resources.\nCreate an administrator user In case you want to use your own account now or later, and you don’t yet have an IAM user with administrator privileges, you need to create one.\nIf you don’t already have an AWS account with Administrator access: create one now by clicking here.\nOnce you have an AWS account, create a new IAM user for this workshop with administrator access to the AWS account: Create a new IAM user to use for the workshop.\nEnter the user details: and select Access Type as Programmatic access. Attach the AdministratorAccess IAM Policy, choose Administrator access or fine-grained access depending upon the Amazon services used for this lab. For this lab you can choose Administrator access → Click Next → Ignore Tags → Next Review. Click to create the new user.\nSuccessfully create Admin User. Congratulations on setting up!",
    "description": "AWS Account and IAM User In order to get started working with AWS services, you will need an AWS account. You will also need an IAM user for this account that you can use to log into the AWS Management Console in order to provision and configure you resources.\nAWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny access to AWS resources.",
    "tags": [],
    "title": "Prepare",
    "uri": "/2-prepare/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "Introduction In today’s lab you will copy the data from a centralized S3 bucket to your AWS account, crawl the dataset with an AWS Glue crawler for metadata creation, transform the data with AWS Glue, query the data and create a View with Athena, and finally build a dashboard with Amazon QuickSight.\nArchitectures Data Ingestion With AWS S3",
    "description": "Introduction In today’s lab you will copy the data from a centralized S3 bucket to your AWS account, crawl the dataset with an AWS Glue crawler for metadata creation, transform the data with AWS Glue, query the data and create a View with Athena, and finally build a dashboard with Amazon QuickSight.\nArchitectures Data Ingestion With AWS S3",
    "tags": [],
    "title": "Data Ingestion",
    "uri": "/3-ingestion/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "Introduction In this lab you will learn about AWS Glue, which is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development. You can use a crawler to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog. Extract, transform, and load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources and targets. The ETL job reads from and writes to the data stores that are specified in the source and target Data Catalog tables.\nPrerequisites Note Data Ingestion with AWS S3 is a prerequisite for this lab.\nSummary In this lab you will be completing the following tasks. You can choose to complete only Data Validation and ETL in order to move to the next lab where tables can be queried using Amazon Athena and Visualize with Amazon Quciksight.\nData Validation and ETL",
    "description": "Introduction In this lab you will learn about AWS Glue, which is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development. You can use a crawler to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog. Extract, transform, and load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources and targets. The ETL job reads from and writes to the data stores that are specified in the source and target Data Catalog tables.",
    "tags": [],
    "title": "Transforming Data",
    "uri": "/4-transforming/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "Introduction This lab introduces you to AWS Glue, Amazon Athena, and Amazon QuickSight. AWS Glue is a fully managed data catalog and ETL service; Amazon Athena provides the ability to run ad-hoc queries on your data in your data lake; and Amazon QuickSight provides visualization of the data you import.\nBelow is a list of the steps for this lab: Athena and QuickSight Athena and SageMaker (Optional)",
    "description": "Introduction This lab introduces you to AWS Glue, Amazon Athena, and Amazon QuickSight. AWS Glue is a fully managed data catalog and ETL service; Amazon Athena provides the ability to run ad-hoc queries on your data in your data lake; and Amazon QuickSight provides visualization of the data you import.\nBelow is a list of the steps for this lab: Athena and QuickSight Athena and SageMaker (Optional)",
    "tags": [],
    "title": "Query and Visualize",
    "uri": "/5-queryandvisualize/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "Cleanup: To avoid unexpected chargers to your account, make sure you clean up your account. To delete the CloudFormation templates using the AWS Management Console -\u003e Login to AWS Management Console and navigate to Services → Management \u0026 Governance → CloudFormation. OR Navigate to AWS CloudFormation\nTo delete the CloudFormation templates using the AWS Management Console → Login to AWS Management Console and navigate Services → Management \u0026 Governance → CloudFormation. Delete the stack from latest to oldest. Note After deleting the CloudFormation stacks if you find any remaining S3 buckets. Remember to delete those S3 buckets.",
    "description": "Cleanup: To avoid unexpected chargers to your account, make sure you clean up your account. To delete the CloudFormation templates using the AWS Management Console -\u003e Login to AWS Management Console and navigate to Services → Management \u0026 Governance → CloudFormation. OR Navigate to AWS CloudFormation\nTo delete the CloudFormation templates using the AWS Management Console → Login to AWS Management Console and navigate Services → Management \u0026 Governance → CloudFormation. Delete the stack from latest to oldest. Note After deleting the CloudFormation stacks if you find any remaining S3 buckets. Remember to delete those S3 buckets.",
    "tags": [],
    "title": "Clean Resource",
    "uri": "/6-cleanresource/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "What Have We Accomplished? Learned about Data Lakes. Learned how to Hydrate the Data Lake. Deployed services to Stream Data Ingestion and Analysis with Kinesis. Deployed services to Batch Data Ingestion with DMS. Learned the working Within the Data Lake with Glue. Deployed services to Transforming data with Glue. Learned to Query and Visualize the Data Lake with Athena and QuickSight. Learned to Automate Data Lake with AWS Lake Formation. Learned to clean and normalize data with DataBrew. Thank you! Thanks a lot for joining the workshop today! We hope you learned something and got inspired.",
    "description": "What Have We Accomplished? Learned about Data Lakes. Learned how to Hydrate the Data Lake. Deployed services to Stream Data Ingestion and Analysis with Kinesis. Deployed services to Batch Data Ingestion with DMS. Learned the working Within the Data Lake with Glue. Deployed services to Transforming data with Glue. Learned to Query and Visualize the Data Lake with Athena and QuickSight. Learned to Automate Data Lake with AWS Lake Formation. Learned to clean and normalize data with DataBrew. Thank you! Thanks a lot for joining the workshop today! We hope you learned something and got inspired.",
    "tags": [],
    "title": "Summary",
    "uri": "/7-summary/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Data Engineering Immersion Day Workshop",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
